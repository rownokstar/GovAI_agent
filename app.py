import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import tempfile

# --- ржкрзЗржЬ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржПржмржВ ржЯрж╛ржЗржЯрзЗрж▓ ---
st.set_page_config(page_title="GovAI Pro (Llama 3)", page_icon="тЪЦя╕П", layout="wide")

st.title("тЪЦя╕П GovAI Pro (Llama 3 Edition)")
st.markdown("""
ржПржЗ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржиржЯрж┐ Llama 3 ржПржмржВ рж╕рж░рзНржмрж╛ржзрзБржирж┐ржХ AI ржкрзНрж░ржпрзБржХрзНрждрж┐ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржпрзЗржХрзЛржирзЛ ржжрзЗрж╢рзЗрж░ ржЖржЗржирж┐ ржбржХрзБржорзЗржирзНржЯ, рж╕ржВржмрж┐ржзрж╛ржи, ржЯрзЗржирзНржбрж╛рж░ ржмрж╛ рж╕рж░ржХрж╛рж░рж┐ ржирзАрждрж┐ржорж╛рж▓рж╛ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рждрзЗ ржкрж╛рж░рзЗред 
ржПржЯрж┐ ржмрж╛ржВрж▓рж╛ ржПржмржВ ржЗржВрж░рзЗржЬрж┐ ржЙржнрзЯ ржнрж╛рж╖рж╛рждрзЗржЗ ржХрж╛ржЬ ржХрж░рждрзЗ рж╕ржХрзНрж╖ржоред
""")

# --- рж╕рж╛ржЗржбржмрж╛рж░: API ржХрзА, ржоржбрзЗрж▓ ржирж┐рж░рзНржмрж╛ржЪржи ржПржмржВ ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб ---
with st.sidebar:
    st.header("ЁЯЫая╕П ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи")
    st.markdown("""
    ржПржЗ ржЕрзНржпрж╛ржкржЯрж┐ ржЪрж╛рж▓рж╛ржирзЛрж░ ржЬржирзНржп ржЖржкржирж╛рж░ ржПржХржЯрж┐ Groq API Key ржкрзНрж░рзЯрзЛржЬржи рж╣ржмрзЗред 
    [ржПржЦрж╛ржи ржерзЗржХрзЗ ржмрж┐ржирж╛ржорзВрж▓рзНржпрзЗ ржЖржкржирж╛рж░ Groq API Key ржирж┐ржи](https://console.groq.com/keys)ред
    """)
    
    # Groq API Key ржЗржиржкрзБржЯ
    groq_api_key = st.text_input("ЁЯФС ржЖржкржирж╛рж░ Groq API Key ржжрж┐ржи", type="password", placeholder="gsk_...")
    
    st.markdown("---")
    
    # LLM ржоржбрзЗрж▓ ржирж┐рж░рзНржмрж╛ржЪржи
    llm_model = st.selectbox(
        "ЁЯза LLM ржоржбрзЗрж▓ ржмрзЗржЫрзЗ ржирж┐ржи",
        ("Llama3-70b-8192", "Llama3-8b-8192"),
        index=0,
        help="70B ржоржбрзЗрж▓ ржмрзЗрж╢рж┐ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржХрж┐ржирзНрждрзБ ржХрж┐ржЫрзБржЯрж╛ ржзрзАрж░ред 8B ржоржбрзЗрж▓ ржЦрзБржм ржжрзНрж░рзБржд ржХрж╛ржЬ ржХрж░рзЗред"
    )
    
    st.markdown("---")
    
    # PDF ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб
    uploaded_file = st.file_uploader("ЁЯУД ржЖржкржирж╛рж░ PDF ржбржХрзБржорзЗржирзНржЯржЯрж┐ ржЖржкрж▓рзЛржб ржХрж░рзБржи (ржмрж╛ржВрж▓рж╛/ржЗржВрж░рзЗржЬрж┐)", type="pdf")
    
    st.markdown("---")
    st.info("ржЖржкржирж╛рж░ ржбрзЗржЯрж╛ рж╕ржорзНржкрзВрж░рзНржг рж╕рзБрж░ржХрзНрж╖рж┐рждред API Key ржмрж╛ ржбржХрзБржорзЗржирзНржЯрзЗрж░ ржХрзЛржирзЛ рждржерзНржп ржЖржорж░рж╛ рж╕ржВрж░ржХрзНрж╖ржг ржХрж░рж┐ ржирж╛ред")

# --- ржХрзНржпрж╛рж╢рж┐ржВ ржлрж╛ржВрж╢ржи (ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржмрж╛рзЬрж╛ржирзЛрж░ ржЬржирзНржп) ---

# ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржПржоржмрзЗржбрж┐ржВ ржоржбрзЗрж▓ржЯрж┐ ржПржХржмрж╛рж░ржЗ рж▓рзЛржб рж╣ржмрзЗ
@st.cache_resource
def load_multilingual_embeddings():
    st.info("ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржПржоржмрзЗржбрж┐ржВ ржоржбрзЗрж▓ (bge-m3) рж▓рзЛржб ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ... (ржкрзНрж░ржержоржмрж╛рж░ ржПржХржЯрзБ рж╕ржорзЯ рж▓рж╛ржЧрждрзЗ ржкрж╛рж░рзЗ)")
    return HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# ржнрзЗржХрзНржЯрж░ рж╕рзНржЯрзЛрж░ рждрзИрж░рж┐ ржПржмржВ ржХрзНржпрж╛рж╢ ржХрж░рж╛
# CORRECTED FUNCTION
@st.cache_data(show_spinner="ржбржХрзБржорзЗржирзНржЯ ржкрзНрж░рж╕рзЗрж╕ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ...")
def create_vector_store(_file_content):
    if not _file_content:
        return None
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(_file_content)
        tmp_file_path = tmp_file.name

    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load_and_split()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
    chunks = text_splitter.split_documents(documents)

    # ржПржоржмрзЗржбрж┐ржВ ржоржбрзЗрж▓ржЯрж┐ ржПржЗ ржлрж╛ржВрж╢ржирзЗрж░ ржнрзЗрждрж░рзЗ ржХрж▓ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ
    embeddings = load_multilingual_embeddings()

    vectorstore = FAISS.from_documents(chunks, embeddings)
    os.remove(tmp_file_path)
    return vectorstore

# --- ржорзВрж▓ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рж▓ржЬрж┐ржХ ---

if not groq_api_key:
    st.warning("ЁЯСИ ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ рж╕рж╛ржЗржбржмрж╛рж░рзЗ ржЖржкржирж╛рж░ Groq API Key ржжрж┐ржиред")
elif not uploaded_file:
    st.warning("ЁЯСИ ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ рж╕рж╛ржЗржбржмрж╛рж░рзЗ ржПржХржЯрж┐ PDF ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб ржХрж░рзБржиред")
else:
    # ржлрж╛ржЗрж▓ ржХржирзНржЯрзЗржирзНржЯ ржкрзЬрж╛
    file_content = uploaded_file.getvalue()
    
    # ржнрзЗржХрзНржЯрж░ рж╕рзНржЯрзЛрж░ рждрзИрж░рж┐ ржмрж╛ ржХрзНржпрж╛рж╢ ржерзЗржХрзЗ рж▓рзЛржб ржХрж░рж╛
    # CORRECTED FUNCTION CALL
    vectorstore = create_vector_store(file_content)
    
    if vectorstore:
        st.success(f"тЬЕ ржбржХрзБржорзЗржирзНржЯ рж╕ржлрж▓ржнрж╛ржмрзЗ ржкрзНрж░рж╕рзЗрж╕ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред ржПржЦржи ржЖржкржирж┐ '{llm_model}' ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рж╢рзНржи ржХрж░рждрзЗ ржкрж╛рж░рзЗржиред")

        # ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ ржкрзНрж░рж╢рзНржи ржЗржиржкрзБржЯ
        query = st.text_input(
            "тЭУ ржбржХрзБржорзЗржирзНржЯ рж╕ржорзНржкрж░рзНржХрзЗ ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржи ржПржЦрж╛ржирзЗ рж▓рж┐ржЦрзБржи (ржмрж╛ржВрж▓рж╛ ржмрж╛ ржЗржВрж░рзЗржЬрж┐рждрзЗ):",
            placeholder="What are the key liabilities for the contractor? / роТрокрпНрокроирпНродржХрж╛рж░рзАрж░ ржорзВрж▓ ржжрж╛ржпрж╝ржмржжрзНржзрждрж╛ржЧрзБрж▓рзЛ ржХрзА ржХрзА?"
        )

        if st.button("╪к╪н┘Д┘К┘Д ржХрж░рзБржи (Analyze)"):
            if not query:
                st.error("ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржПржХржЯрж┐ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржиред")
            else:
                with st.spinner(f"Llama 3 ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржЦрзБржБржЬржЫрзЗ... ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржЕржкрзЗржХрзНрж╖рж╛ ржХрж░рзБржиред"):
                    try:
                        # LLM ржоржбрзЗрж▓ ржЗржирж┐рж╢рж┐рзЯрж╛рж▓рж╛ржЗржЬ ржХрж░рж╛
                        llm = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=llm_model)

                        # ржмрж╣рзБржнрж╛рж╖рж┐ржХ ржкрзНрж░ржорзНржкржЯ ржЯрзЗржоржкрзНрж▓рзЗржЯ
                        prompt_template = """
                        You are "GovAI Pro", a highly intelligent legal and policy analysis AI.
                        Your task is to answer the user's question based ONLY on the provided context from a legal or government document.
                        The document and the query can be in either English or Bengali. You MUST answer in the same language as the user's query.

                        Your answer must be structured into three distinct sections:
                        1.  **Summary (рж╕рж╛рж░рж╛ржВрж╢):** Provide a clear and concise answer to the user's question in simple language.
                        2.  **Relevant Clauses (рж╕ржорзНржкрж░рзНржХрж┐ржд ржзрж╛рж░рж╛):** Mention the specific clause or section numbers from the document that support your answer.
                        3.  **Potential Risks/Key Points (рж╕ржорзНржнрж╛ржмрзНржп ржЭрзБржБржХрж┐/ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржмрж┐рж╖ржпрж╝):** Highlight any risks, penalties, deadlines, obligations, or inconsistencies mentioned in the context related to the query.

                        If the answer is not found in the provided context, you MUST state: "The provided context does not contain information on this topic. / ржкрзНрж░ржжрждрзНржд ржХржиржЯрзЗржХрзНрж╕ржЯрзЗ ржПржЗ ржмрж┐рж╖ржпрж╝рзЗ ржХрзЛржирзЛ рждржерзНржп ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ред"

                        CONTEXT:
                        {context}

                        QUERY:
                        {question}

                        Follow these instructions and provide a structured, factual answer.
                        """
                        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

                        # RetrievalQA ржЪрзЗржЗржи рждрзИрж░рж┐ ржХрж░рж╛
                        qa_chain = RetrievalQA.from_chain_type(
                            llm=llm,
                            chain_type="stuff",
                            retriever=vectorstore.as_retriever(),
                            chain_type_kwargs={"prompt": PROMPT}
                        )

                        # ржлрж▓рж╛ржлрж▓ ржЬрзЗржирж╛рж░рзЗржЯ ржХрж░рж╛
                        result = qa_chain.invoke({"query": query})

                        # ржлрж▓рж╛ржлрж▓ ржкрзНрж░ржжрж░рзНрж╢ржи
                        st.subheader("ЁЯУД GovAI Pro ржмрж┐рж╢рзНрж▓рзЗрж╖ржг:")
                        st.markdown(result["result"])

                    except Exception as e:
                        st.error(f"ржПржХржЯрж┐ рждрзНрж░рзБржЯрж┐ ржШржЯрзЗржЫрзЗ: {e}")
                        st.info("ржЖржкржирж╛рж░ Groq API Key рж╕ржарж┐ржХ ржХрж┐ржирж╛ ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи ржЕржержмрж╛ ржЕржирзНржп ржХрзЛржирзЛ ржкрзНрж░рж╢рзНржи ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржиред")
