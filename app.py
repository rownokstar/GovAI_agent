import streamlit as st
import os
import time
from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.document_loaders import PyPDFLoader, CSVLoader, Docx2txtLoader, UnstructuredExcelLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from fpdf import FPDF
from docx import Document as DocxDocument
import pandas as pd
import io

# --- ‡¶™‡ßá‡¶ú ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ---
st.set_page_config(page_title="GovAI Pro 2.0", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ GovAI Pro 2.0 - ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡ßá‡¶ï‡ßç‡¶ü‡¶ø‡¶≠ ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶ü")

# --- ‡¶∏‡¶æ‡¶á‡¶°‡¶¨‡¶æ‡¶∞ ---
with st.sidebar:
    st.header("üõ†Ô∏è ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶®")
    st.markdown("API Key-‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶ñ‡¶® Streamlit Secrets ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡ßç‡¶¨‡¶Ø‡¶º‡¶Ç‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶≠‡¶æ‡¶¨‡ßá ‡¶≤‡ßã‡¶° ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§")
    
    if st.button("üí¨ ‡¶®‡¶§‡ßÅ‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®", use_container_width=True):
        st.session_state.clear()
        st.rerun()

    st.markdown("---")
    st.info("‡¶è‡¶á ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡¶ü‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ, ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§")


# --- API ‡¶ï‡ßÄ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ---
groq_api_key = st.secrets.get("GROQ_API_KEY")
openai_api_key = st.secrets.get("OPENAI_API_KEY")

if not groq_api_key or not openai_api_key:
    st.error("‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá Streamlit Cloud-‡¶è‡¶∞ Settings > Secrets ‡¶∏‡ßá‡¶ï‡¶∂‡¶®‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Groq ‡¶è‡¶¨‡¶Ç OpenAI API Key ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")
    st.stop()

# --- ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶∏‡¶Æ‡ßÇ‡¶π ---

@st.cache_resource(show_spinner="‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá... ‚è≥")
def get_vectorstore_from_file(uploaded_file):
    try:
        file_extension = os.path.splitext(uploaded_file.name)[1]
        temp_file_path = f"./{uploaded_file.name}"
        
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        loaders = {
            ".pdf": PyPDFLoader,
            ".docx": Docx2txtLoader,
            ".csv": CSVLoader,
            ".xls": UnstructuredExcelLoader,
            ".xlsx": UnstructuredExcelLoader
        }
        loader_class = loaders.get(file_extension)

        if loader_class is None:
            st.error("‡¶è‡¶á ‡¶´‡¶æ‡¶á‡¶≤ ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡¶ü‡¶ø ‡¶∏‡¶æ‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ‡•§")
            return None
            
        loader = loader_class(temp_file_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        document_chunks = text_splitter.split_documents(documents)
        
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vector_store = FAISS.from_documents(document_chunks, embeddings)

        os.remove(temp_file_path)
        return vector_store
    except Exception as e:
        st.error(f"‡¶´‡¶æ‡¶á‡¶≤ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç-‡¶è ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá: {e}")
        return None

def get_history_aware_retriever(llm, retriever):
    history_aware_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and a follow-up question, rephrase the question to be a standalone question. **Crucially, the rephrased question MUST be in the same language as the original follow-up question (English, Bengali, or Banglish).** Do not translate the question."),
        ("user", "{input}"),
    ])
    return create_history_aware_retriever(llm, retriever, history_aware_prompt)

def get_conversational_rag_chain(history_aware_retriever):
    llm = ChatGroq(temperature=0.2, groq_api_key=groq_api_key, model_name="Llama3-70b-8192")
    
    conversational_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful and friendly AI assistant named GovAI Pro. Answer the user's questions based on the provided context and conversation history.

Language Rules:
- If the user asks in English, respond in English.
- If the user asks in Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ), respond in clear Bengali.
- If the user asks in Banglish (e.g., "amar kisu document lagbe"), understand it as Bengali and respond in clear Bengali.

Formatting Rules:
- Use appropriate emojis to make the conversation engaging. üëç
- If you need to present structured data (like comparisons, lists of items, etc.), use Markdown tables.
- If you don't know the answer from the context, politely say so in the correct language.

Context:
{context}"""),
        ("user", "{input}"),
    ])
    
    stuff_documents_chain = create_stuff_documents_chain(llm, conversational_prompt)
    return create_retrieval_chain(history_aware_retriever, stuff_documents_chain)

# --- ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶ü ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ---
def generate_pdf_report(chat_history):
    pdf = FPDF()
    pdf.add_page()
    try:
        # This path works in Streamlit Cloud's default environment
        pdf.add_font('DejaVu', '', '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', uni=True)
        pdf.set_font('DejaVu', '', 12)
    except RuntimeError:
        # Fallback for local environments that might not have the font
        pdf.set_font('Arial', '', 12)
        pdf.cell(0, 10, "Warning: DejaVuSans font not found. Using Arial (non-Unicode).", ln=True)

    pdf.cell(0, 10, txt="GovAI Pro - Chat Report", ln=True, align='C')
    for message in chat_history:
        role = "User" if isinstance(message, HumanMessage) else "Assistant"
        # Encode to a basic encoding and decode back to handle potential problematic characters
        content = message.content.encode('latin-1', 'replace').decode('latin-1')
        pdf.multi_cell(0, 10, f"{role}: {content}")
        pdf.ln(5)
    
    # CORRECTED LINE: pdf.output() now returns bytes directly.
    return pdf.output()

def generate_docx_report(chat_history):
    document = DocxDocument()
    document.add_heading('GovAI Pro - Chat Report', 0)
    for message in chat_history:
        role = "User" if isinstance(message, HumanMessage) else "Assistant"
        document.add_paragraph(f"{role}: {message.content}")
    bio = io.BytesIO()
    document.save(bio)
    return bio.getvalue()

def generate_excel_report(chat_history):
    data = {"Role": [], "Message": []}
    for message in chat_history:
        role = "User" if isinstance(message, HumanMessage) else "Assistant"
        data["Role"].append(role)
        data["Message"].append(message.content)
    df = pd.DataFrame(data)
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Chat History')
    return bio.getvalue()

# --- ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßç‡¶ü‡ßá‡¶ü ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶æ ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [AIMessage(content="‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡¶Ü‡¶Æ‡¶ø GovAI Pro‡•§ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶∏‡¶æ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")]
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

# --- ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ---
uploaded_file = st.file_uploader(
    "üìÑ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶® (PDF, DOCX, CSV, Excel)", 
    type=['pdf', 'docx', 'csv', 'xls', 'xlsx'],
    help="‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡¶∞ ‡¶Ü‡¶™‡¶®‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶®‡•§"
)

if uploaded_file:
    if st.session_state.get("uploaded_file_name") != uploaded_file.name:
        st.session_state.vector_store = get_vectorstore_from_file(uploaded_file)
        st.session_state.uploaded_file_name = uploaded_file.name

if st.session_state.vector_store is None:
    st.info("‡¶è‡¶ï‡¶ü‡¶ø ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")
else:
    # --- ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø ‡¶°‡¶ø‡¶∏‡¶™‡ßç‡¶≤‡ßá ---
    for message in st.session_state.chat_history:
        if isinstance(message, AIMessage):
            with st.chat_message("AI", avatar="ü§ñ"):
                st.write(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human", avatar="üë§"):
                st.write(message.content)

    # --- ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ---
    user_query = st.chat_input("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®...")
    if user_query:
        st.session_state.chat_history.append(HumanMessage(content=user_query))
        with st.chat_message("Human", avatar="üë§"):
            st.write(user_query)

        with st.chat_message("AI", avatar="ü§ñ"):
            llm = ChatGroq(temperature=0.2, groq_api_key=groq_api_key, model_name="Llama3-8b-8192")
            retriever = st.session_state.vector_store.as_retriever()
            history_aware_retriever = get_history_aware_retriever(llm, retriever)
            rag_chain = get_conversational_rag_chain(history_aware_retriever)
            
            def stream_chunks():
                response_stream = rag_chain.stream({
                    "input": user_query,
                    "chat_history": st.session_state.chat_history
                })
                for chunk in response_stream:
                    if "answer" in chunk:
                        yield chunk["answer"]
                        time.sleep(0.02)
            
            full_response = st.write_stream(stream_chunks)
            st.session_state.chat_history.append(AIMessage(content=full_response))

            # --- ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶¨‡¶æ‡¶ü‡¶® ---
            st.markdown("---")
            st.write("‡¶è‡¶á ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ‡¶ü‡¶ø ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®:")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.download_button("üìÑ PDF", generate_pdf_report(st.session_state.chat_history), "chat_report.pdf", "application/pdf", use_container_width=True)
            with col2:
                st.download_button("üìë DOCX", generate_docx_report(st.session_state.chat_history), "chat_report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document", use_container_width=True)
            with col3:
                st.download_button("üìä Excel", generate_excel_report(st.session_state.chat_history), "chat_report.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", use_container_width=True)
