import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import tempfile

# --- ржкрзЗржЬ ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи ржПржмржВ ржЯрж╛ржЗржЯрзЗрж▓ ---
st.set_page_config(page_title="GovAI Pro (Llama 3)", page_icon="тЪЦя╕П", layout="wide")

st.title("тЪЦя╕П GovAI Pro (API Edition)")
st.markdown("""
ржПржЗ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржиржЯрж┐ Llama 3 ржПржмржВ OpenAI-ржПрж░ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржПржоржмрзЗржбрж┐ржВ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржпрзЗржХрзЛржирзЛ ржжрзЗрж╢рзЗрж░ ржЖржЗржирж┐ ржбржХрзБржорзЗржирзНржЯ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рзЗред
ржПржЯрж┐ ржПржЦржи рж╕ржорзНржкрзВрж░рзНржг API-ржирж┐рж░рзНржнрж░, рждрж╛ржЗ ржорзЗржорзЛрж░рж┐ ржХрзНрж░рзНржпрж╛рж╢рзЗрж░ ржХрзЛржирзЛ рж╕ржорзНржнрж╛ржмржирж╛ ржирзЗржЗред
""")

# --- рж╕рж╛ржЗржбржмрж╛рж░: API ржХрзА, ржоржбрзЗрж▓ ржирж┐рж░рзНржмрж╛ржЪржи ржПржмржВ ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб ---
with st.sidebar:
    st.header("ЁЯЫая╕П ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи")
    st.markdown("ржПржЗ ржЕрзНржпрж╛ржкржЯрж┐ ржЪрж╛рж▓рж╛ржирзЛрж░ ржЬржирзНржп ржЖржкржирж╛рж░ ржжрзБржЯрж┐ API Key ржкрзНрж░рзЯрзЛржЬржи рж╣ржмрзЗред")
    
    # Groq API Key
    groq_api_key = st.text_input("ЁЯФС Groq API Key (Llama 3-ржПрж░ ржЬржирзНржп)", type="password", placeholder="gsk_...")
    
    # OpenAI API Key
    openai_api_key = st.text_input("ЁЯФС OpenAI API Key (Embedding-ржПрж░ ржЬржирзНржп)", type="password", placeholder="sk_...")

    st.markdown("---")
    
    # LLM ржоржбрзЗрж▓ ржирж┐рж░рзНржмрж╛ржЪржи
    llm_model = st.selectbox(
        "ЁЯза LLM ржоржбрзЗрж▓ ржмрзЗржЫрзЗ ржирж┐ржи",
        ("Llama3-70b-8192", "Llama3-8b-8192"),
        index=0
    )
    
    st.markdown("---")
    
    # PDF ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб
    uploaded_file = st.file_uploader("ЁЯУД ржЖржкржирж╛рж░ PDF ржбржХрзБржорзЗржирзНржЯржЯрж┐ ржЖржкрж▓рзЛржб ржХрж░рзБржи", type="pdf")
    
    st.markdown("---")
    st.info("ржЖржкржирж╛рж░ ржбрзЗржЯрж╛ рж╕ржорзНржкрзВрж░рзНржг рж╕рзБрж░ржХрзНрж╖рж┐рждред")

# --- ржХрзНржпрж╛рж╢рж┐ржВ ржлрж╛ржВрж╢ржи ---

# ржнрзЗржХрзНржЯрж░ рж╕рзНржЯрзЛрж░ рждрзИрж░рж┐ ржПржмржВ ржХрзНржпрж╛рж╢ ржХрж░рж╛
@st.cache_data(show_spinner="ржбржХрзБржорзЗржирзНржЯ ржкрзНрж░рж╕рзЗрж╕ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ...")
def create_vector_store(_file_content, _openai_api_key):
    if not _file_content:
        return None
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(_file_content)
        tmp_file_path = tmp_file.name

    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load_and_split()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    
    # OpenAI Embedding API ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=_openai_api_key)
        vectorstore = FAISS.from_documents(chunks, embeddings)
    except Exception as e:
        st.error(f"ржПржоржмрзЗржбрж┐ржВ рждрзИрж░рж┐рж░ рж╕ржоржпрж╝ рждрзНрж░рзБржЯрж┐: {e}")
        return None

    os.remove(tmp_file_path)
    return vectorstore

# --- ржорзВрж▓ ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи рж▓ржЬрж┐ржХ ---

if not groq_api_key or not openai_api_key:
    st.warning("ЁЯСИ ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ рж╕рж╛ржЗржбржмрж╛рж░рзЗ ржЖржкржирж╛рж░ Groq ржПржмржВ OpenAI API Key ржжрзБржЯрж┐ржЗ ржжрж┐ржиред")
elif not uploaded_file:
    st.warning("ЁЯСИ ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ рж╕рж╛ржЗржбржмрж╛рж░рзЗ ржПржХржЯрж┐ PDF ржлрж╛ржЗрж▓ ржЖржкрж▓рзЛржб ржХрж░рзБржиред")
else:
    file_content = uploaded_file.getvalue()
    vectorstore = create_vector_store(file_content, openai_api_key)
    
    if vectorstore:
        st.success(f"тЬЕ ржбржХрзБржорзЗржирзНржЯ рж╕ржлрж▓ржнрж╛ржмрзЗ ржкрзНрж░рж╕рзЗрж╕ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред ржПржЦржи ржЖржкржирж┐ '{llm_model}' ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рж╢рзНржи ржХрж░рждрзЗ ржкрж╛рж░рзЗржиред")
        query = st.text_input(
            "тЭУ ржбржХрзБржорзЗржирзНржЯ рж╕ржорзНржкрж░рзНржХрзЗ ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржи (ржмрж╛ржВрж▓рж╛ ржмрж╛ ржЗржВрж░рзЗржЬрж┐рждрзЗ):",
            placeholder="What are the key liabilities? / ржорзВрж▓ ржжрж╛ржпрж╝ржмржжрзНржзрждрж╛ржЧрзБрж▓рзЛ ржХрзА ржХрзА?"
        )

        if st.button("╪к╪н┘Д┘К┘Д ржХрж░рзБржи (Analyze)"):
            if not query:
                st.error("ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржПржХржЯрж┐ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржиред")
            else:
                with st.spinner(f"Llama 3 ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржЦрзБржБржЬржЫрзЗ..."):
                    try:
                        llm = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=llm_model)
                        
                        retriever = vectorstore.as_retriever()
                        
                        prompt_template = """
                        You are "GovAI Pro". Your task is to answer the user's question based ONLY on the provided context.
                        The document and query can be in English or Bengali. You MUST answer in the same language as the user's query.

                        Structure your answer in three sections:
                        1.  **Summary (рж╕рж╛рж░рж╛ржВрж╢):** Clear answer to the query.
                        2.  **Relevant Clauses (рж╕ржорзНржкрж░рзНржХрж┐ржд ржзрж╛рж░рж╛):** Mention the clause numbers.
                        3.  **Potential Risks/Key Points (рж╕ржорзНржнрж╛ржмрзНржп ржЭрзБржБржХрж┐/ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржмрж┐рж╖ржпрж╝):** Highlight risks, penalties, etc.

                        If the answer is not in the context, state: "The provided context does not contain information on this topic. / ржкрзНрж░ржжрждрзНржд ржХржиржЯрзЗржХрзНрж╕ржЯрзЗ ржПржЗ ржмрж┐рж╖ржпрж╝рзЗ ржХрзЛржирзЛ рждржерзНржп ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ред"

                        CONTEXT: {context}
                        QUERY: {question}
                        
                        Answer:
                        """
                        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

                        qa_chain = RetrievalQA.from_chain_type(
                            llm=llm,
                            chain_type="stuff",
                            retriever=retriever,
                            chain_type_kwargs={"prompt": PROMPT}
                        )

                        result = qa_chain.invoke({"query": query})
                        st.subheader("ЁЯУД GovAI Pro ржмрж┐рж╢рзНрж▓рзЗрж╖ржг:")
                        st.markdown(result["result"])

                    except Exception as e:
                        st.error(f"ржПржХржЯрж┐ рждрзНрж░рзБржЯрж┐ ржШржЯрзЗржЫрзЗ: {e}")
